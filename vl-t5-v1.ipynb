{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2055693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, TrainerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a1ffac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a816139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_semtypes(file_path):\n",
    "    data = [line.split() for line in open(file_path)]\n",
    "    data = [[t[0], \" \".join(t[1:])] for t in data]\n",
    "    data = pd.DataFrame(data)\n",
    "    data.columns = [\"id\", \"semtypes\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55c381d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptioningDataset(Dataset):\n",
    "    def __init__(self, image_dir, data_file, semtypes_file, tokenizer):\n",
    "        self.image_dir = image_dir        \n",
    "        self.data = pd.read_csv(data_file)\n",
    "        self.semtypes = load_semtypes(semtypes_file)\n",
    "\n",
    "        # TODO (REMOVE DOWNSIZING)\n",
    "        self.data = self.data[:int(len(self.data) * 0.001)]\n",
    "        self.semtypes = self.semtypes[:int(len(self.semtypes) * 0.001)]\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.image_dir, self.data.loc[index, 'name'])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        caption = self.data.loc[index, 'caption']\n",
    "        semtypes = self.semtypes.loc[index, 'semtypes']\n",
    "        encoding = self.tokenizer.encode_plus(caption, semtypes, padding='longest', truncation=True)\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': torch.tensor(encoding['input_ids']),\n",
    "            'attention_mask': torch.tensor(encoding['attention_mask']),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "100c9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLT5Model(T5ForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.visual_projection = nn.Linear(2048, config.d_model)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        image_features = self.image_encoder(image)\n",
    "        projected_features = self.visual_projection(image_features)\n",
    "\n",
    "        input_ids = torch.cat((projected_features, input_ids[:, 1:]), dim=1)\n",
    "        attention_mask = torch.cat((torch.ones_like(projected_features[:, :1]), attention_mask[:, 1:]), dim=1)\n",
    "\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids.long(),\n",
    "            attention_mask=attention_mask.long(),\n",
    "            decoder_input_ids=input_ids.long(),\n",
    "            decoder_attention_mask=attention_mask.long()\n",
    "        )\n",
    "\n",
    "        return outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2021e51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohith/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/rohith/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Some weights of VLT5Model were not initialized from the model checkpoint at t5-base and are newly initialized: ['image_encoder.resnet.conv1.weight', 'image_encoder.resnet.bn1.weight', 'image_encoder.resnet.bn1.bias', 'image_encoder.resnet.bn1.running_mean', 'image_encoder.resnet.bn1.running_var', 'image_encoder.resnet.layer1.0.conv1.weight', 'image_encoder.resnet.layer1.0.bn1.weight', 'image_encoder.resnet.layer1.0.bn1.bias', 'image_encoder.resnet.layer1.0.bn1.running_mean', 'image_encoder.resnet.layer1.0.bn1.running_var', 'image_encoder.resnet.layer1.0.conv2.weight', 'image_encoder.resnet.layer1.0.bn2.weight', 'image_encoder.resnet.layer1.0.bn2.bias', 'image_encoder.resnet.layer1.0.bn2.running_mean', 'image_encoder.resnet.layer1.0.bn2.running_var', 'image_encoder.resnet.layer1.0.conv3.weight', 'image_encoder.resnet.layer1.0.bn3.weight', 'image_encoder.resnet.layer1.0.bn3.bias', 'image_encoder.resnet.layer1.0.bn3.running_mean', 'image_encoder.resnet.layer1.0.bn3.running_var', 'image_encoder.resnet.layer1.0.downsample.0.weight', 'image_encoder.resnet.layer1.0.downsample.1.weight', 'image_encoder.resnet.layer1.0.downsample.1.bias', 'image_encoder.resnet.layer1.0.downsample.1.running_mean', 'image_encoder.resnet.layer1.0.downsample.1.running_var', 'image_encoder.resnet.layer1.1.conv1.weight', 'image_encoder.resnet.layer1.1.bn1.weight', 'image_encoder.resnet.layer1.1.bn1.bias', 'image_encoder.resnet.layer1.1.bn1.running_mean', 'image_encoder.resnet.layer1.1.bn1.running_var', 'image_encoder.resnet.layer1.1.conv2.weight', 'image_encoder.resnet.layer1.1.bn2.weight', 'image_encoder.resnet.layer1.1.bn2.bias', 'image_encoder.resnet.layer1.1.bn2.running_mean', 'image_encoder.resnet.layer1.1.bn2.running_var', 'image_encoder.resnet.layer1.1.conv3.weight', 'image_encoder.resnet.layer1.1.bn3.weight', 'image_encoder.resnet.layer1.1.bn3.bias', 'image_encoder.resnet.layer1.1.bn3.running_mean', 'image_encoder.resnet.layer1.1.bn3.running_var', 'image_encoder.resnet.layer1.2.conv1.weight', 'image_encoder.resnet.layer1.2.bn1.weight', 'image_encoder.resnet.layer1.2.bn1.bias', 'image_encoder.resnet.layer1.2.bn1.running_mean', 'image_encoder.resnet.layer1.2.bn1.running_var', 'image_encoder.resnet.layer1.2.conv2.weight', 'image_encoder.resnet.layer1.2.bn2.weight', 'image_encoder.resnet.layer1.2.bn2.bias', 'image_encoder.resnet.layer1.2.bn2.running_mean', 'image_encoder.resnet.layer1.2.bn2.running_var', 'image_encoder.resnet.layer1.2.conv3.weight', 'image_encoder.resnet.layer1.2.bn3.weight', 'image_encoder.resnet.layer1.2.bn3.bias', 'image_encoder.resnet.layer1.2.bn3.running_mean', 'image_encoder.resnet.layer1.2.bn3.running_var', 'image_encoder.resnet.layer2.0.conv1.weight', 'image_encoder.resnet.layer2.0.bn1.weight', 'image_encoder.resnet.layer2.0.bn1.bias', 'image_encoder.resnet.layer2.0.bn1.running_mean', 'image_encoder.resnet.layer2.0.bn1.running_var', 'image_encoder.resnet.layer2.0.conv2.weight', 'image_encoder.resnet.layer2.0.bn2.weight', 'image_encoder.resnet.layer2.0.bn2.bias', 'image_encoder.resnet.layer2.0.bn2.running_mean', 'image_encoder.resnet.layer2.0.bn2.running_var', 'image_encoder.resnet.layer2.0.conv3.weight', 'image_encoder.resnet.layer2.0.bn3.weight', 'image_encoder.resnet.layer2.0.bn3.bias', 'image_encoder.resnet.layer2.0.bn3.running_mean', 'image_encoder.resnet.layer2.0.bn3.running_var', 'image_encoder.resnet.layer2.0.downsample.0.weight', 'image_encoder.resnet.layer2.0.downsample.1.weight', 'image_encoder.resnet.layer2.0.downsample.1.bias', 'image_encoder.resnet.layer2.0.downsample.1.running_mean', 'image_encoder.resnet.layer2.0.downsample.1.running_var', 'image_encoder.resnet.layer2.1.conv1.weight', 'image_encoder.resnet.layer2.1.bn1.weight', 'image_encoder.resnet.layer2.1.bn1.bias', 'image_encoder.resnet.layer2.1.bn1.running_mean', 'image_encoder.resnet.layer2.1.bn1.running_var', 'image_encoder.resnet.layer2.1.conv2.weight', 'image_encoder.resnet.layer2.1.bn2.weight', 'image_encoder.resnet.layer2.1.bn2.bias', 'image_encoder.resnet.layer2.1.bn2.running_mean', 'image_encoder.resnet.layer2.1.bn2.running_var', 'image_encoder.resnet.layer2.1.conv3.weight', 'image_encoder.resnet.layer2.1.bn3.weight', 'image_encoder.resnet.layer2.1.bn3.bias', 'image_encoder.resnet.layer2.1.bn3.running_mean', 'image_encoder.resnet.layer2.1.bn3.running_var', 'image_encoder.resnet.layer2.2.conv1.weight', 'image_encoder.resnet.layer2.2.bn1.weight', 'image_encoder.resnet.layer2.2.bn1.bias', 'image_encoder.resnet.layer2.2.bn1.running_mean', 'image_encoder.resnet.layer2.2.bn1.running_var', 'image_encoder.resnet.layer2.2.conv2.weight', 'image_encoder.resnet.layer2.2.bn2.weight', 'image_encoder.resnet.layer2.2.bn2.bias', 'image_encoder.resnet.layer2.2.bn2.running_mean', 'image_encoder.resnet.layer2.2.bn2.running_var', 'image_encoder.resnet.layer2.2.conv3.weight', 'image_encoder.resnet.layer2.2.bn3.weight', 'image_encoder.resnet.layer2.2.bn3.bias', 'image_encoder.resnet.layer2.2.bn3.running_mean', 'image_encoder.resnet.layer2.2.bn3.running_var', 'image_encoder.resnet.layer2.3.conv1.weight', 'image_encoder.resnet.layer2.3.bn1.weight', 'image_encoder.resnet.layer2.3.bn1.bias', 'image_encoder.resnet.layer2.3.bn1.running_mean', 'image_encoder.resnet.layer2.3.bn1.running_var', 'image_encoder.resnet.layer2.3.conv2.weight', 'image_encoder.resnet.layer2.3.bn2.weight', 'image_encoder.resnet.layer2.3.bn2.bias', 'image_encoder.resnet.layer2.3.bn2.running_mean', 'image_encoder.resnet.layer2.3.bn2.running_var', 'image_encoder.resnet.layer2.3.conv3.weight', 'image_encoder.resnet.layer2.3.bn3.weight', 'image_encoder.resnet.layer2.3.bn3.bias', 'image_encoder.resnet.layer2.3.bn3.running_mean', 'image_encoder.resnet.layer2.3.bn3.running_var', 'image_encoder.resnet.layer3.0.conv1.weight', 'image_encoder.resnet.layer3.0.bn1.weight', 'image_encoder.resnet.layer3.0.bn1.bias', 'image_encoder.resnet.layer3.0.bn1.running_mean', 'image_encoder.resnet.layer3.0.bn1.running_var', 'image_encoder.resnet.layer3.0.conv2.weight', 'image_encoder.resnet.layer3.0.bn2.weight', 'image_encoder.resnet.layer3.0.bn2.bias', 'image_encoder.resnet.layer3.0.bn2.running_mean', 'image_encoder.resnet.layer3.0.bn2.running_var', 'image_encoder.resnet.layer3.0.conv3.weight', 'image_encoder.resnet.layer3.0.bn3.weight', 'image_encoder.resnet.layer3.0.bn3.bias', 'image_encoder.resnet.layer3.0.bn3.running_mean', 'image_encoder.resnet.layer3.0.bn3.running_var', 'image_encoder.resnet.layer3.0.downsample.0.weight', 'image_encoder.resnet.layer3.0.downsample.1.weight', 'image_encoder.resnet.layer3.0.downsample.1.bias', 'image_encoder.resnet.layer3.0.downsample.1.running_mean', 'image_encoder.resnet.layer3.0.downsample.1.running_var', 'image_encoder.resnet.layer3.1.conv1.weight', 'image_encoder.resnet.layer3.1.bn1.weight', 'image_encoder.resnet.layer3.1.bn1.bias', 'image_encoder.resnet.layer3.1.bn1.running_mean', 'image_encoder.resnet.layer3.1.bn1.running_var', 'image_encoder.resnet.layer3.1.conv2.weight', 'image_encoder.resnet.layer3.1.bn2.weight', 'image_encoder.resnet.layer3.1.bn2.bias', 'image_encoder.resnet.layer3.1.bn2.running_mean', 'image_encoder.resnet.layer3.1.bn2.running_var', 'image_encoder.resnet.layer3.1.conv3.weight', 'image_encoder.resnet.layer3.1.bn3.weight', 'image_encoder.resnet.layer3.1.bn3.bias', 'image_encoder.resnet.layer3.1.bn3.running_mean', 'image_encoder.resnet.layer3.1.bn3.running_var', 'image_encoder.resnet.layer3.2.conv1.weight', 'image_encoder.resnet.layer3.2.bn1.weight', 'image_encoder.resnet.layer3.2.bn1.bias', 'image_encoder.resnet.layer3.2.bn1.running_mean', 'image_encoder.resnet.layer3.2.bn1.running_var', 'image_encoder.resnet.layer3.2.conv2.weight', 'image_encoder.resnet.layer3.2.bn2.weight', 'image_encoder.resnet.layer3.2.bn2.bias', 'image_encoder.resnet.layer3.2.bn2.running_mean', 'image_encoder.resnet.layer3.2.bn2.running_var', 'image_encoder.resnet.layer3.2.conv3.weight', 'image_encoder.resnet.layer3.2.bn3.weight', 'image_encoder.resnet.layer3.2.bn3.bias', 'image_encoder.resnet.layer3.2.bn3.running_mean', 'image_encoder.resnet.layer3.2.bn3.running_var', 'image_encoder.resnet.layer3.3.conv1.weight', 'image_encoder.resnet.layer3.3.bn1.weight', 'image_encoder.resnet.layer3.3.bn1.bias', 'image_encoder.resnet.layer3.3.bn1.running_mean', 'image_encoder.resnet.layer3.3.bn1.running_var', 'image_encoder.resnet.layer3.3.conv2.weight', 'image_encoder.resnet.layer3.3.bn2.weight', 'image_encoder.resnet.layer3.3.bn2.bias', 'image_encoder.resnet.layer3.3.bn2.running_mean', 'image_encoder.resnet.layer3.3.bn2.running_var', 'image_encoder.resnet.layer3.3.conv3.weight', 'image_encoder.resnet.layer3.3.bn3.weight', 'image_encoder.resnet.layer3.3.bn3.bias', 'image_encoder.resnet.layer3.3.bn3.running_mean', 'image_encoder.resnet.layer3.3.bn3.running_var', 'image_encoder.resnet.layer3.4.conv1.weight', 'image_encoder.resnet.layer3.4.bn1.weight', 'image_encoder.resnet.layer3.4.bn1.bias', 'image_encoder.resnet.layer3.4.bn1.running_mean', 'image_encoder.resnet.layer3.4.bn1.running_var', 'image_encoder.resnet.layer3.4.conv2.weight', 'image_encoder.resnet.layer3.4.bn2.weight', 'image_encoder.resnet.layer3.4.bn2.bias', 'image_encoder.resnet.layer3.4.bn2.running_mean', 'image_encoder.resnet.layer3.4.bn2.running_var', 'image_encoder.resnet.layer3.4.conv3.weight', 'image_encoder.resnet.layer3.4.bn3.weight', 'image_encoder.resnet.layer3.4.bn3.bias', 'image_encoder.resnet.layer3.4.bn3.running_mean', 'image_encoder.resnet.layer3.4.bn3.running_var', 'image_encoder.resnet.layer3.5.conv1.weight', 'image_encoder.resnet.layer3.5.bn1.weight', 'image_encoder.resnet.layer3.5.bn1.bias', 'image_encoder.resnet.layer3.5.bn1.running_mean', 'image_encoder.resnet.layer3.5.bn1.running_var', 'image_encoder.resnet.layer3.5.conv2.weight', 'image_encoder.resnet.layer3.5.bn2.weight', 'image_encoder.resnet.layer3.5.bn2.bias', 'image_encoder.resnet.layer3.5.bn2.running_mean', 'image_encoder.resnet.layer3.5.bn2.running_var', 'image_encoder.resnet.layer3.5.conv3.weight', 'image_encoder.resnet.layer3.5.bn3.weight', 'image_encoder.resnet.layer3.5.bn3.bias', 'image_encoder.resnet.layer3.5.bn3.running_mean', 'image_encoder.resnet.layer3.5.bn3.running_var', 'image_encoder.resnet.layer4.0.conv1.weight', 'image_encoder.resnet.layer4.0.bn1.weight', 'image_encoder.resnet.layer4.0.bn1.bias', 'image_encoder.resnet.layer4.0.bn1.running_mean', 'image_encoder.resnet.layer4.0.bn1.running_var', 'image_encoder.resnet.layer4.0.conv2.weight', 'image_encoder.resnet.layer4.0.bn2.weight', 'image_encoder.resnet.layer4.0.bn2.bias', 'image_encoder.resnet.layer4.0.bn2.running_mean', 'image_encoder.resnet.layer4.0.bn2.running_var', 'image_encoder.resnet.layer4.0.conv3.weight', 'image_encoder.resnet.layer4.0.bn3.weight', 'image_encoder.resnet.layer4.0.bn3.bias', 'image_encoder.resnet.layer4.0.bn3.running_mean', 'image_encoder.resnet.layer4.0.bn3.running_var', 'image_encoder.resnet.layer4.0.downsample.0.weight', 'image_encoder.resnet.layer4.0.downsample.1.weight', 'image_encoder.resnet.layer4.0.downsample.1.bias', 'image_encoder.resnet.layer4.0.downsample.1.running_mean', 'image_encoder.resnet.layer4.0.downsample.1.running_var', 'image_encoder.resnet.layer4.1.conv1.weight', 'image_encoder.resnet.layer4.1.bn1.weight', 'image_encoder.resnet.layer4.1.bn1.bias', 'image_encoder.resnet.layer4.1.bn1.running_mean', 'image_encoder.resnet.layer4.1.bn1.running_var', 'image_encoder.resnet.layer4.1.conv2.weight', 'image_encoder.resnet.layer4.1.bn2.weight', 'image_encoder.resnet.layer4.1.bn2.bias', 'image_encoder.resnet.layer4.1.bn2.running_mean', 'image_encoder.resnet.layer4.1.bn2.running_var', 'image_encoder.resnet.layer4.1.conv3.weight', 'image_encoder.resnet.layer4.1.bn3.weight', 'image_encoder.resnet.layer4.1.bn3.bias', 'image_encoder.resnet.layer4.1.bn3.running_mean', 'image_encoder.resnet.layer4.1.bn3.running_var', 'image_encoder.resnet.layer4.2.conv1.weight', 'image_encoder.resnet.layer4.2.bn1.weight', 'image_encoder.resnet.layer4.2.bn1.bias', 'image_encoder.resnet.layer4.2.bn1.running_mean', 'image_encoder.resnet.layer4.2.bn1.running_var', 'image_encoder.resnet.layer4.2.conv2.weight', 'image_encoder.resnet.layer4.2.bn2.weight', 'image_encoder.resnet.layer4.2.bn2.bias', 'image_encoder.resnet.layer4.2.bn2.running_mean', 'image_encoder.resnet.layer4.2.bn2.running_var', 'image_encoder.resnet.layer4.2.conv3.weight', 'image_encoder.resnet.layer4.2.bn3.weight', 'image_encoder.resnet.layer4.2.bn3.bias', 'image_encoder.resnet.layer4.2.bn3.running_mean', 'image_encoder.resnet.layer4.2.bn3.running_var', 'visual_projection.weight', 'visual_projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = VLT5Model.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "20aa9978",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir = './all_data/train/radiology/images/'\n",
    "valid_image_dir = './all_data/validation/radiology/images/'\n",
    "train_data_file = './all_data/train/radiology/traindata.csv'\n",
    "valid_data_file = './all_data/validation/radiology/valdata.csv'\n",
    "train_semtypes_file = './all_data/train/radiology/semtypes.txt'\n",
    "valid_semtypes_file = './all_data/validation/radiology/semtypes.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a2ba2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CaptioningDataset(train_image_dir, train_data_file, train_semtypes_file, tokenizer)\n",
    "valid_dataset = CaptioningDataset(valid_image_dir, valid_data_file, valid_semtypes_file, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "634fee9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "28a898fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./output',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=50,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7d61ee78",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wb/_rgrp9kx0wsds433bv835rc40000gn/T/ipykernel_88849/2738127379.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./trained_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wb/_rgrp9kx0wsds433bv835rc40000gn/T/ipykernel_88849/2344077387.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, image)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojected_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         outputs = super().forward(\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m             \u001b[0;31m# Convert encoder inputs in embeddings if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1498\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, encoder_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You have to initialize the model with valid token embeddings\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model('./trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f5dab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
