{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2055693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, TrainerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a1ffac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a816139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_semtypes(file_path):\n",
    "    data = [line.split() for line in open(file_path)]\n",
    "    data = [[t[0], \" \".join(t[1:])] for t in data]\n",
    "    data = pd.DataFrame(data)\n",
    "    data.columns = [\"id\", \"semtypes\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c381d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptioningDataset(Dataset):\n",
    "    def __init__(self, image_dir, data_file, semtypes_file, tokenizer):\n",
    "        self.image_dir = image_dir        \n",
    "        self.data = pd.read_csv(data_file)\n",
    "        self.semtypes = load_semtypes(semtypes_file)\n",
    "\n",
    "        # TODO (REMOVE DOWNSIZING)\n",
    "        self.data = self.data[:int(len(self.data) * 0.001)]\n",
    "        self.semtypes = self.semtypes[:int(len(self.semtypes) * 0.001)]\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.image_dir, self.data.loc[index, 'name'])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        caption = self.data.loc[index, 'caption']\n",
    "        semtypes = self.semtypes.loc[index, 'semtypes']\n",
    "        encoding = self.tokenizer.encode_plus(caption, semtypes, padding='max_length', max_length=128, truncation=True)\n",
    "#         encoding = self.tokenizer.encode_plus(caption, semtypes, padding='longest')\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': torch.tensor(encoding['input_ids']),\n",
    "            'attention_mask': torch.tensor(encoding['attention_mask']),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100c9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLT5Model(T5ForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.visual_projection = nn.Linear(2048, config.d_model//6)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        image_features = self.image_encoder(image)\n",
    "        projected_features = self.visual_projection(image_features)\n",
    "        \n",
    "#         print(input_ids)\n",
    "\n",
    "#         input_ids = torch.cat((projected_features, input_ids[:, 1:]), dim=1)\n",
    "#         attention_mask = torch.cat((torch.ones_like(projected_features[:, :1]), attention_mask[:, 1:]), dim=1)\n",
    "\n",
    "#         outputs = super().forward(\n",
    "#             input_ids=input_ids.long(),\n",
    "#             attention_mask=attention_mask.long(),\n",
    "#             decoder_input_ids=input_ids.long(),\n",
    "#             decoder_attention_mask=attention_mask.long()\n",
    "#         )\n",
    "\n",
    "        print(input_ids.shape)\n",
    "        print(attention_mask.shape)\n",
    "        print(projected_features.shape)\n",
    "\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_outputs=(projected_features,),\n",
    "            decoder_input_ids=input_ids,\n",
    "            decoder_attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        return outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2021e51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohith/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/rohith/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Some weights of VLT5Model were not initialized from the model checkpoint at t5-base and are newly initialized: ['image_encoder.resnet.conv1.weight', 'image_encoder.resnet.bn1.weight', 'image_encoder.resnet.bn1.bias', 'image_encoder.resnet.bn1.running_mean', 'image_encoder.resnet.bn1.running_var', 'image_encoder.resnet.layer1.0.conv1.weight', 'image_encoder.resnet.layer1.0.bn1.weight', 'image_encoder.resnet.layer1.0.bn1.bias', 'image_encoder.resnet.layer1.0.bn1.running_mean', 'image_encoder.resnet.layer1.0.bn1.running_var', 'image_encoder.resnet.layer1.0.conv2.weight', 'image_encoder.resnet.layer1.0.bn2.weight', 'image_encoder.resnet.layer1.0.bn2.bias', 'image_encoder.resnet.layer1.0.bn2.running_mean', 'image_encoder.resnet.layer1.0.bn2.running_var', 'image_encoder.resnet.layer1.0.conv3.weight', 'image_encoder.resnet.layer1.0.bn3.weight', 'image_encoder.resnet.layer1.0.bn3.bias', 'image_encoder.resnet.layer1.0.bn3.running_mean', 'image_encoder.resnet.layer1.0.bn3.running_var', 'image_encoder.resnet.layer1.0.downsample.0.weight', 'image_encoder.resnet.layer1.0.downsample.1.weight', 'image_encoder.resnet.layer1.0.downsample.1.bias', 'image_encoder.resnet.layer1.0.downsample.1.running_mean', 'image_encoder.resnet.layer1.0.downsample.1.running_var', 'image_encoder.resnet.layer1.1.conv1.weight', 'image_encoder.resnet.layer1.1.bn1.weight', 'image_encoder.resnet.layer1.1.bn1.bias', 'image_encoder.resnet.layer1.1.bn1.running_mean', 'image_encoder.resnet.layer1.1.bn1.running_var', 'image_encoder.resnet.layer1.1.conv2.weight', 'image_encoder.resnet.layer1.1.bn2.weight', 'image_encoder.resnet.layer1.1.bn2.bias', 'image_encoder.resnet.layer1.1.bn2.running_mean', 'image_encoder.resnet.layer1.1.bn2.running_var', 'image_encoder.resnet.layer1.1.conv3.weight', 'image_encoder.resnet.layer1.1.bn3.weight', 'image_encoder.resnet.layer1.1.bn3.bias', 'image_encoder.resnet.layer1.1.bn3.running_mean', 'image_encoder.resnet.layer1.1.bn3.running_var', 'image_encoder.resnet.layer1.2.conv1.weight', 'image_encoder.resnet.layer1.2.bn1.weight', 'image_encoder.resnet.layer1.2.bn1.bias', 'image_encoder.resnet.layer1.2.bn1.running_mean', 'image_encoder.resnet.layer1.2.bn1.running_var', 'image_encoder.resnet.layer1.2.conv2.weight', 'image_encoder.resnet.layer1.2.bn2.weight', 'image_encoder.resnet.layer1.2.bn2.bias', 'image_encoder.resnet.layer1.2.bn2.running_mean', 'image_encoder.resnet.layer1.2.bn2.running_var', 'image_encoder.resnet.layer1.2.conv3.weight', 'image_encoder.resnet.layer1.2.bn3.weight', 'image_encoder.resnet.layer1.2.bn3.bias', 'image_encoder.resnet.layer1.2.bn3.running_mean', 'image_encoder.resnet.layer1.2.bn3.running_var', 'image_encoder.resnet.layer2.0.conv1.weight', 'image_encoder.resnet.layer2.0.bn1.weight', 'image_encoder.resnet.layer2.0.bn1.bias', 'image_encoder.resnet.layer2.0.bn1.running_mean', 'image_encoder.resnet.layer2.0.bn1.running_var', 'image_encoder.resnet.layer2.0.conv2.weight', 'image_encoder.resnet.layer2.0.bn2.weight', 'image_encoder.resnet.layer2.0.bn2.bias', 'image_encoder.resnet.layer2.0.bn2.running_mean', 'image_encoder.resnet.layer2.0.bn2.running_var', 'image_encoder.resnet.layer2.0.conv3.weight', 'image_encoder.resnet.layer2.0.bn3.weight', 'image_encoder.resnet.layer2.0.bn3.bias', 'image_encoder.resnet.layer2.0.bn3.running_mean', 'image_encoder.resnet.layer2.0.bn3.running_var', 'image_encoder.resnet.layer2.0.downsample.0.weight', 'image_encoder.resnet.layer2.0.downsample.1.weight', 'image_encoder.resnet.layer2.0.downsample.1.bias', 'image_encoder.resnet.layer2.0.downsample.1.running_mean', 'image_encoder.resnet.layer2.0.downsample.1.running_var', 'image_encoder.resnet.layer2.1.conv1.weight', 'image_encoder.resnet.layer2.1.bn1.weight', 'image_encoder.resnet.layer2.1.bn1.bias', 'image_encoder.resnet.layer2.1.bn1.running_mean', 'image_encoder.resnet.layer2.1.bn1.running_var', 'image_encoder.resnet.layer2.1.conv2.weight', 'image_encoder.resnet.layer2.1.bn2.weight', 'image_encoder.resnet.layer2.1.bn2.bias', 'image_encoder.resnet.layer2.1.bn2.running_mean', 'image_encoder.resnet.layer2.1.bn2.running_var', 'image_encoder.resnet.layer2.1.conv3.weight', 'image_encoder.resnet.layer2.1.bn3.weight', 'image_encoder.resnet.layer2.1.bn3.bias', 'image_encoder.resnet.layer2.1.bn3.running_mean', 'image_encoder.resnet.layer2.1.bn3.running_var', 'image_encoder.resnet.layer2.2.conv1.weight', 'image_encoder.resnet.layer2.2.bn1.weight', 'image_encoder.resnet.layer2.2.bn1.bias', 'image_encoder.resnet.layer2.2.bn1.running_mean', 'image_encoder.resnet.layer2.2.bn1.running_var', 'image_encoder.resnet.layer2.2.conv2.weight', 'image_encoder.resnet.layer2.2.bn2.weight', 'image_encoder.resnet.layer2.2.bn2.bias', 'image_encoder.resnet.layer2.2.bn2.running_mean', 'image_encoder.resnet.layer2.2.bn2.running_var', 'image_encoder.resnet.layer2.2.conv3.weight', 'image_encoder.resnet.layer2.2.bn3.weight', 'image_encoder.resnet.layer2.2.bn3.bias', 'image_encoder.resnet.layer2.2.bn3.running_mean', 'image_encoder.resnet.layer2.2.bn3.running_var', 'image_encoder.resnet.layer2.3.conv1.weight', 'image_encoder.resnet.layer2.3.bn1.weight', 'image_encoder.resnet.layer2.3.bn1.bias', 'image_encoder.resnet.layer2.3.bn1.running_mean', 'image_encoder.resnet.layer2.3.bn1.running_var', 'image_encoder.resnet.layer2.3.conv2.weight', 'image_encoder.resnet.layer2.3.bn2.weight', 'image_encoder.resnet.layer2.3.bn2.bias', 'image_encoder.resnet.layer2.3.bn2.running_mean', 'image_encoder.resnet.layer2.3.bn2.running_var', 'image_encoder.resnet.layer2.3.conv3.weight', 'image_encoder.resnet.layer2.3.bn3.weight', 'image_encoder.resnet.layer2.3.bn3.bias', 'image_encoder.resnet.layer2.3.bn3.running_mean', 'image_encoder.resnet.layer2.3.bn3.running_var', 'image_encoder.resnet.layer3.0.conv1.weight', 'image_encoder.resnet.layer3.0.bn1.weight', 'image_encoder.resnet.layer3.0.bn1.bias', 'image_encoder.resnet.layer3.0.bn1.running_mean', 'image_encoder.resnet.layer3.0.bn1.running_var', 'image_encoder.resnet.layer3.0.conv2.weight', 'image_encoder.resnet.layer3.0.bn2.weight', 'image_encoder.resnet.layer3.0.bn2.bias', 'image_encoder.resnet.layer3.0.bn2.running_mean', 'image_encoder.resnet.layer3.0.bn2.running_var', 'image_encoder.resnet.layer3.0.conv3.weight', 'image_encoder.resnet.layer3.0.bn3.weight', 'image_encoder.resnet.layer3.0.bn3.bias', 'image_encoder.resnet.layer3.0.bn3.running_mean', 'image_encoder.resnet.layer3.0.bn3.running_var', 'image_encoder.resnet.layer3.0.downsample.0.weight', 'image_encoder.resnet.layer3.0.downsample.1.weight', 'image_encoder.resnet.layer3.0.downsample.1.bias', 'image_encoder.resnet.layer3.0.downsample.1.running_mean', 'image_encoder.resnet.layer3.0.downsample.1.running_var', 'image_encoder.resnet.layer3.1.conv1.weight', 'image_encoder.resnet.layer3.1.bn1.weight', 'image_encoder.resnet.layer3.1.bn1.bias', 'image_encoder.resnet.layer3.1.bn1.running_mean', 'image_encoder.resnet.layer3.1.bn1.running_var', 'image_encoder.resnet.layer3.1.conv2.weight', 'image_encoder.resnet.layer3.1.bn2.weight', 'image_encoder.resnet.layer3.1.bn2.bias', 'image_encoder.resnet.layer3.1.bn2.running_mean', 'image_encoder.resnet.layer3.1.bn2.running_var', 'image_encoder.resnet.layer3.1.conv3.weight', 'image_encoder.resnet.layer3.1.bn3.weight', 'image_encoder.resnet.layer3.1.bn3.bias', 'image_encoder.resnet.layer3.1.bn3.running_mean', 'image_encoder.resnet.layer3.1.bn3.running_var', 'image_encoder.resnet.layer3.2.conv1.weight', 'image_encoder.resnet.layer3.2.bn1.weight', 'image_encoder.resnet.layer3.2.bn1.bias', 'image_encoder.resnet.layer3.2.bn1.running_mean', 'image_encoder.resnet.layer3.2.bn1.running_var', 'image_encoder.resnet.layer3.2.conv2.weight', 'image_encoder.resnet.layer3.2.bn2.weight', 'image_encoder.resnet.layer3.2.bn2.bias', 'image_encoder.resnet.layer3.2.bn2.running_mean', 'image_encoder.resnet.layer3.2.bn2.running_var', 'image_encoder.resnet.layer3.2.conv3.weight', 'image_encoder.resnet.layer3.2.bn3.weight', 'image_encoder.resnet.layer3.2.bn3.bias', 'image_encoder.resnet.layer3.2.bn3.running_mean', 'image_encoder.resnet.layer3.2.bn3.running_var', 'image_encoder.resnet.layer3.3.conv1.weight', 'image_encoder.resnet.layer3.3.bn1.weight', 'image_encoder.resnet.layer3.3.bn1.bias', 'image_encoder.resnet.layer3.3.bn1.running_mean', 'image_encoder.resnet.layer3.3.bn1.running_var', 'image_encoder.resnet.layer3.3.conv2.weight', 'image_encoder.resnet.layer3.3.bn2.weight', 'image_encoder.resnet.layer3.3.bn2.bias', 'image_encoder.resnet.layer3.3.bn2.running_mean', 'image_encoder.resnet.layer3.3.bn2.running_var', 'image_encoder.resnet.layer3.3.conv3.weight', 'image_encoder.resnet.layer3.3.bn3.weight', 'image_encoder.resnet.layer3.3.bn3.bias', 'image_encoder.resnet.layer3.3.bn3.running_mean', 'image_encoder.resnet.layer3.3.bn3.running_var', 'image_encoder.resnet.layer3.4.conv1.weight', 'image_encoder.resnet.layer3.4.bn1.weight', 'image_encoder.resnet.layer3.4.bn1.bias', 'image_encoder.resnet.layer3.4.bn1.running_mean', 'image_encoder.resnet.layer3.4.bn1.running_var', 'image_encoder.resnet.layer3.4.conv2.weight', 'image_encoder.resnet.layer3.4.bn2.weight', 'image_encoder.resnet.layer3.4.bn2.bias', 'image_encoder.resnet.layer3.4.bn2.running_mean', 'image_encoder.resnet.layer3.4.bn2.running_var', 'image_encoder.resnet.layer3.4.conv3.weight', 'image_encoder.resnet.layer3.4.bn3.weight', 'image_encoder.resnet.layer3.4.bn3.bias', 'image_encoder.resnet.layer3.4.bn3.running_mean', 'image_encoder.resnet.layer3.4.bn3.running_var', 'image_encoder.resnet.layer3.5.conv1.weight', 'image_encoder.resnet.layer3.5.bn1.weight', 'image_encoder.resnet.layer3.5.bn1.bias', 'image_encoder.resnet.layer3.5.bn1.running_mean', 'image_encoder.resnet.layer3.5.bn1.running_var', 'image_encoder.resnet.layer3.5.conv2.weight', 'image_encoder.resnet.layer3.5.bn2.weight', 'image_encoder.resnet.layer3.5.bn2.bias', 'image_encoder.resnet.layer3.5.bn2.running_mean', 'image_encoder.resnet.layer3.5.bn2.running_var', 'image_encoder.resnet.layer3.5.conv3.weight', 'image_encoder.resnet.layer3.5.bn3.weight', 'image_encoder.resnet.layer3.5.bn3.bias', 'image_encoder.resnet.layer3.5.bn3.running_mean', 'image_encoder.resnet.layer3.5.bn3.running_var', 'image_encoder.resnet.layer4.0.conv1.weight', 'image_encoder.resnet.layer4.0.bn1.weight', 'image_encoder.resnet.layer4.0.bn1.bias', 'image_encoder.resnet.layer4.0.bn1.running_mean', 'image_encoder.resnet.layer4.0.bn1.running_var', 'image_encoder.resnet.layer4.0.conv2.weight', 'image_encoder.resnet.layer4.0.bn2.weight', 'image_encoder.resnet.layer4.0.bn2.bias', 'image_encoder.resnet.layer4.0.bn2.running_mean', 'image_encoder.resnet.layer4.0.bn2.running_var', 'image_encoder.resnet.layer4.0.conv3.weight', 'image_encoder.resnet.layer4.0.bn3.weight', 'image_encoder.resnet.layer4.0.bn3.bias', 'image_encoder.resnet.layer4.0.bn3.running_mean', 'image_encoder.resnet.layer4.0.bn3.running_var', 'image_encoder.resnet.layer4.0.downsample.0.weight', 'image_encoder.resnet.layer4.0.downsample.1.weight', 'image_encoder.resnet.layer4.0.downsample.1.bias', 'image_encoder.resnet.layer4.0.downsample.1.running_mean', 'image_encoder.resnet.layer4.0.downsample.1.running_var', 'image_encoder.resnet.layer4.1.conv1.weight', 'image_encoder.resnet.layer4.1.bn1.weight', 'image_encoder.resnet.layer4.1.bn1.bias', 'image_encoder.resnet.layer4.1.bn1.running_mean', 'image_encoder.resnet.layer4.1.bn1.running_var', 'image_encoder.resnet.layer4.1.conv2.weight', 'image_encoder.resnet.layer4.1.bn2.weight', 'image_encoder.resnet.layer4.1.bn2.bias', 'image_encoder.resnet.layer4.1.bn2.running_mean', 'image_encoder.resnet.layer4.1.bn2.running_var', 'image_encoder.resnet.layer4.1.conv3.weight', 'image_encoder.resnet.layer4.1.bn3.weight', 'image_encoder.resnet.layer4.1.bn3.bias', 'image_encoder.resnet.layer4.1.bn3.running_mean', 'image_encoder.resnet.layer4.1.bn3.running_var', 'image_encoder.resnet.layer4.2.conv1.weight', 'image_encoder.resnet.layer4.2.bn1.weight', 'image_encoder.resnet.layer4.2.bn1.bias', 'image_encoder.resnet.layer4.2.bn1.running_mean', 'image_encoder.resnet.layer4.2.bn1.running_var', 'image_encoder.resnet.layer4.2.conv2.weight', 'image_encoder.resnet.layer4.2.bn2.weight', 'image_encoder.resnet.layer4.2.bn2.bias', 'image_encoder.resnet.layer4.2.bn2.running_mean', 'image_encoder.resnet.layer4.2.bn2.running_var', 'image_encoder.resnet.layer4.2.conv3.weight', 'image_encoder.resnet.layer4.2.bn3.weight', 'image_encoder.resnet.layer4.2.bn3.bias', 'image_encoder.resnet.layer4.2.bn3.running_mean', 'image_encoder.resnet.layer4.2.bn3.running_var', 'visual_projection.weight', 'visual_projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = VLT5Model.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20aa9978",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir = './all_data/train/radiology/images/'\n",
    "valid_image_dir = './all_data/validation/radiology/images/'\n",
    "train_data_file = './all_data/train/radiology/traindata.csv'\n",
    "valid_data_file = './all_data/validation/radiology/valdata.csv'\n",
    "train_semtypes_file = './all_data/train/radiology/semtypes.txt'\n",
    "valid_semtypes_file = './all_data/validation/radiology/semtypes.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2ba2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CaptioningDataset(train_image_dir, train_data_file, train_semtypes_file, tokenizer)\n",
    "valid_dataset = CaptioningDataset(valid_image_dir, valid_data_file, valid_semtypes_file, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "634fee9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28a898fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./output',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=50,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d61ee78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([8, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (8x128 and 768x768)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wb/_rgrp9kx0wsds433bv835rc40000gn/T/ipykernel_99446/2738127379.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./trained_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wb/_rgrp9kx0wsds433bv835rc40000gn/T/ipykernel_99446/1131121010.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, image)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojected_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         outputs = super().forward(\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1544\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, encoder_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    944\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    947\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, encoder_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0mquery_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m             cross_attention_outputs = self.layer[1](\n\u001b[0m\u001b[1;32m    660\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m                 \u001b[0mkey_value_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    572\u001b[0m     ):\n\u001b[1;32m    573\u001b[0m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         attention_output = self.EncDecAttention(\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;31m# get key/value states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         key_states = project(\n\u001b[0m\u001b[1;32m    472\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_value_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mproject\u001b[0;34m(hidden_states, proj_layer, key_value_states, past_key_value)\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0;31m# cross-attn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0;31m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproj_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_value_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x128 and 768x768)"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model('./trained_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
